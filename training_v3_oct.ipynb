{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1kgN-PLEsO_CBvm3-Y2p6UFAZNIB8axfW",
      "authorship_tag": "ABX9TyMuGJtVBNZYsCOUqeKVtHHT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PiyushTewari2/squeezenet-pytorch/blob/main/training_v3_oct.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaSwddYkptlQ",
        "outputId": "89efd85e-fb89-476e-9ec1-a97a952fcfe1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SqueezeNet(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
            "    (3): Fire(\n",
            "      (squeeze): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
            "    (5): Fire(\n",
            "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
            "    (7): Fire(\n",
            "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (1): Flatten(start_dim=1, end_dim=-1)\n",
            "    (2): Linear(in_features=384, out_features=2, bias=True)\n",
            "    (3): Softmax(dim=1)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.nn.init as init\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import zipfile\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "\n",
        "use_gpu = torch.cuda.is_available()\n",
        "\n",
        "# Unzipping the dataset\n",
        "zip_file = '/content/drive/MyDrive/squeeze-net/Data2.zip'\n",
        "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/data')\n",
        "\n",
        "# Data paths\n",
        "train_dir = '/content/data/Data/Train_Data'\n",
        "test_dir = '/content/data/Data/Test_Data'\n",
        "\n",
        "# Data transformations\n",
        "data_transforms = {\n",
        "    'Train_Data': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor()\n",
        "        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'Test_Data': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor()\n",
        "        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# Datasets and Dataloaders\n",
        "image_datasets = {\n",
        "    'Train_Data': datasets.ImageFolder(root=train_dir, transform=data_transforms['Train_Data']),\n",
        "    'Test_Data': datasets.ImageFolder(root=test_dir, transform=data_transforms['Test_Data'])\n",
        "}\n",
        "\n",
        "dataloaders = {\n",
        "    'Train_Data': DataLoader(image_datasets['Train_Data'], batch_size=4, shuffle=True),\n",
        "    'Test_Data': DataLoader(image_datasets['Test_Data'], batch_size=4, shuffle=False)\n",
        "}\n",
        "\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['Train_Data', 'Test_Data']}\n",
        "class_names = image_datasets['Train_Data'].classes\n",
        "\n",
        "# Define the SqueezeNet v1.1 model explicitly\n",
        "class Fire(nn.Module):\n",
        "    def __init__(self, inplanes, squeeze_planes, expand1x1_planes, expand3x3_planes):\n",
        "        super(Fire, self).__init__()\n",
        "        self.inplanes = inplanes\n",
        "        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n",
        "        self.squeeze_activation = nn.ReLU(inplace=True)\n",
        "        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes, kernel_size=1)\n",
        "        self.expand1x1_activation = nn.ReLU(inplace=True)\n",
        "        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes, kernel_size=3, padding=1)\n",
        "        self.expand3x3_activation = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.squeeze_activation(self.squeeze(x))\n",
        "        return torch.cat([\n",
        "            self.expand1x1_activation(self.expand1x1(x)),\n",
        "            self.expand3x3_activation(self.expand3x3(x))\n",
        "        ], 1)\n",
        "\n",
        "class SqueezeNet(nn.Module):\n",
        "    def __init__(self, version='1_1', num_classes=1000):\n",
        "        super(SqueezeNet, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        if version == '1_1':\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(3, 64, kernel_size=3, stride=2),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(64, 16, 64, 64),\n",
        "                #Fire(128, 16, 64, 64),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(128, 32, 128, 128),\n",
        "                #Fire(256, 32, 128, 128),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(256, 48, 192, 192),\n",
        "                #Fire(384, 48, 192, 192),\n",
        "                #Fire(384, 64, 256, 256),\n",
        "                #Fire(512, 64, 256, 256),\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported SqueezeNet version {version}: 1_0 or 1_1 expected\".format(version=version))\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(output_size=(1, 1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(384, num_classes),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "        # Initialize weights\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "              init.kaiming_uniform_(m.weight)\n",
        "              if m.bias is not None:\n",
        "                  init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Create the SqueezeNet model for binary classification\n",
        "squeezenet1_1 = SqueezeNet(version='1_1', num_classes=2)\n",
        "print(squeezenet1_1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Move the model to GPU if available\n",
        "if use_gpu:\n",
        "    squeezenet1_1 = squeezenet1_1.cuda()\n",
        "\n",
        "# Define loss criterion, optimizer, and learning rate scheduler\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_ft = optim.SGD(squeezenet1_1.parameters(), lr=0.0005)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
        "\n",
        "# Training loop\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=10):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['Train_Data', 'Test_Data']:\n",
        "            if phase == 'Train_Data':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                if use_gpu:\n",
        "                    inputs = inputs.cuda()\n",
        "                    labels = labels.cuda()\n",
        "\n",
        "                # Zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward\n",
        "                with torch.set_grad_enabled(phase == 'Train_Data'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # Backward + optimize only if in training phase\n",
        "                    if phase == 'Train_Data':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # Statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            if phase == 'Train_Data':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            # Deep copy the model\n",
        "            if phase == 'Test_Data' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "    # Load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "# Train the model\n",
        "squeezenet1_1 = train_model(squeezenet1_1, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=20)\n",
        "torch.save(squeezenet1_1.state_dict(), '/content/drive/MyDrive/squeezenet_v4_nov.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abSY0qBJrNW6",
        "outputId": "bec8b9ed-01ee-4117-c89f-79dd8a03832c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "----------\n",
            "Train_Data Loss: 0.4647 Acc: 0.8594\n",
            "Test_Data Loss: 0.4541 Acc: 0.8629\n",
            "\n",
            "Epoch 2/20\n",
            "----------\n",
            "Train_Data Loss: 0.4418 Acc: 0.8751\n",
            "Test_Data Loss: 0.4372 Acc: 0.8705\n",
            "\n",
            "Epoch 3/20\n",
            "----------\n",
            "Train_Data Loss: 0.4265 Acc: 0.8853\n",
            "Test_Data Loss: 0.4241 Acc: 0.8803\n",
            "\n",
            "Epoch 4/20\n",
            "----------\n",
            "Train_Data Loss: 0.4143 Acc: 0.8981\n",
            "Test_Data Loss: 0.4088 Acc: 0.9021\n",
            "\n",
            "Epoch 5/20\n",
            "----------\n",
            "Train_Data Loss: 0.4023 Acc: 0.9128\n",
            "Test_Data Loss: 0.4052 Acc: 0.9032\n",
            "\n",
            "Epoch 6/20\n",
            "----------\n",
            "Train_Data Loss: 0.3961 Acc: 0.9185\n",
            "Test_Data Loss: 0.3952 Acc: 0.9140\n",
            "\n",
            "Epoch 7/20\n",
            "----------\n",
            "Train_Data Loss: 0.3921 Acc: 0.9232\n",
            "Test_Data Loss: 0.3892 Acc: 0.9391\n",
            "\n",
            "Epoch 8/20\n",
            "----------\n",
            "Train_Data Loss: 0.3866 Acc: 0.9289\n",
            "Test_Data Loss: 0.3844 Acc: 0.9260\n",
            "\n",
            "Epoch 9/20\n",
            "----------\n",
            "Train_Data Loss: 0.3858 Acc: 0.9294\n",
            "Test_Data Loss: 0.3827 Acc: 0.9304\n",
            "\n",
            "Epoch 10/20\n",
            "----------\n",
            "Train_Data Loss: 0.3856 Acc: 0.9327\n",
            "Test_Data Loss: 0.3824 Acc: 0.9282\n",
            "\n",
            "Epoch 11/20\n",
            "----------\n",
            "Train_Data Loss: 0.3853 Acc: 0.9307\n",
            "Test_Data Loss: 0.3820 Acc: 0.9314\n",
            "\n",
            "Epoch 12/20\n",
            "----------\n",
            "Train_Data Loss: 0.3854 Acc: 0.9280\n",
            "Test_Data Loss: 0.3822 Acc: 0.9304\n",
            "\n",
            "Epoch 13/20\n",
            "----------\n",
            "Train_Data Loss: 0.3847 Acc: 0.9291\n",
            "Test_Data Loss: 0.3819 Acc: 0.9304\n",
            "\n",
            "Epoch 14/20\n",
            "----------\n",
            "Train_Data Loss: 0.3848 Acc: 0.9289\n",
            "Test_Data Loss: 0.3812 Acc: 0.9314\n",
            "\n",
            "Epoch 15/20\n",
            "----------\n",
            "Train_Data Loss: 0.3839 Acc: 0.9316\n",
            "Test_Data Loss: 0.3812 Acc: 0.9304\n",
            "\n",
            "Epoch 16/20\n",
            "----------\n",
            "Train_Data Loss: 0.3839 Acc: 0.9320\n",
            "Test_Data Loss: 0.3812 Acc: 0.9304\n",
            "\n",
            "Epoch 17/20\n",
            "----------\n",
            "Train_Data Loss: 0.3841 Acc: 0.9316\n",
            "Test_Data Loss: 0.3812 Acc: 0.9293\n",
            "\n",
            "Epoch 18/20\n",
            "----------\n",
            "Train_Data Loss: 0.3839 Acc: 0.9311\n",
            "Test_Data Loss: 0.3812 Acc: 0.9293\n",
            "\n",
            "Epoch 19/20\n",
            "----------\n",
            "Train_Data Loss: 0.3838 Acc: 0.9325\n",
            "Test_Data Loss: 0.3813 Acc: 0.9304\n",
            "\n",
            "Epoch 20/20\n",
            "----------\n",
            "Train_Data Loss: 0.3838 Acc: 0.9314\n",
            "Test_Data Loss: 0.3811 Acc: 0.9293\n",
            "\n",
            "Training complete in 16m 50s\n",
            "Best val Acc: 0.939064\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "def quantize_to_9bit_signed(weight):\n",
        "    # Quantize to 9-bit signed integers\n",
        "    scale = 2 ** 8\n",
        "    quantized_weight = np.round(weight * scale).astype(np.int16)\n",
        "    quantized_weight = np.clip(quantized_weight, -256, 255)  # Clip to fit in 9 bits\n",
        "    return quantized_weight\n",
        "\n",
        "def format_weights_for_verilog(weights, num_channels, num_filters):\n",
        "    formatted_weights = []\n",
        "    for c in range(num_channels - 1, -1, -1):  # Channels in reverse order\n",
        "        channel_weights = []\n",
        "        for f in range(num_filters - 1, -1, -1):  # Filters in reverse order\n",
        "            filter_weights = weights[f, c].flatten().tolist()\n",
        "            filter_weights_formatted = ','.join([f\"{'-' if w < 0 else ''}9'd{abs(w)}\" for w in filter_weights])\n",
        "            channel_weights.append(f\"{{{filter_weights_formatted}}}\")\n",
        "        formatted_weights.append(f\"{{{','.join(channel_weights)}}}\")\n",
        "\n",
        "    return f\"input_weight = {{{','.join(formatted_weights)}}};\"\n",
        "\n",
        "def format_biases_for_verilog(bias):\n",
        "    bias_formatted = ','.join([f\"{'-' if b < 0 else ''}9'd{abs(b)}\" for b in bias])\n",
        "    return f\"input_bias = {{{bias_formatted}}};\"\n",
        "\n",
        "def save_quantized_weights_and_biases(model, output_dir, zip_filename):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Process each layer's weights and biases\n",
        "    for name, param in model.named_parameters():\n",
        "        param_np = param.detach().cpu().numpy()\n",
        "        quantized_param = quantize_to_9bit_signed(param_np)\n",
        "\n",
        "        if 'weight' in name:\n",
        "            if len(quantized_param.shape) == 4:\n",
        "                # For 4D convolutional weights\n",
        "                num_filters, num_channels, _, _ = quantized_param.shape\n",
        "                formatted_weights = format_weights_for_verilog(quantized_param, num_channels, num_filters)\n",
        "            elif len(quantized_param.shape) == 2:\n",
        "                # For 2D linear weights\n",
        "                num_filters, num_channels = quantized_param.shape\n",
        "                formatted_weights = format_weights_for_verilog(quantized_param.reshape(num_filters, num_channels, 1, 1), num_channels, num_filters)\n",
        "\n",
        "            output_file = os.path.join(output_dir, f\"{name.replace('.', '_')}.txt\")\n",
        "            with open(output_file, 'w') as f:\n",
        "                f.write(formatted_weights)\n",
        "\n",
        "        elif 'bias' in name:\n",
        "            formatted_bias = format_biases_for_verilog(quantized_param)\n",
        "            output_file = os.path.join(output_dir, f\"{name.replace('.', '_')}.txt\")\n",
        "            with open(output_file, 'w') as f:\n",
        "                f.write(formatted_bias)\n",
        "\n",
        "    # Zip the output directory\n",
        "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, _, files in os.walk(output_dir):\n",
        "            for file in files:\n",
        "                zipf.write(os.path.join(root, file), arcname=os.path.relpath(os.path.join(root, file), output_dir))\n",
        "\n",
        "# Load the model and the weights\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SqueezeNet(version='1_1', num_classes=2).to(device)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/squeezenet_v3_oct.pt', map_location=device))\n",
        "\n",
        "# Save quantized weights and biases in Verilog format\n",
        "save_quantized_weights_and_biases(model, '/content/drive/MyDrive/quantized_weights_v3', '/content/drive/MyDrive/quantized_weights_v3.zip')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCg7HMaS0y2y",
        "outputId": "831e915e-616d-43fe-dbc3-8925387b63c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-d87d083b54f0>:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('/content/drive/MyDrive/squeezenet_v3_oct.pt', map_location=device))\n"
          ]
        }
      ]
    }
  ]
}